{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "228cc688-aaee-42c7-846c-ae07302c6297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##Controle de versões\n",
    "##Objetivo:\n",
    "Esse notebook tem a função de realizar ingestão de files\n",
    "\n",
    "-------------\n",
    "\n",
    "###**Histórico de versões:**\n",
    "| Data | Desenvolvido por | Modificações |\n",
    "|---|---|---|\n",
    "|09/04/2025|João Guilherme Brandi|Criação do notebook |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd4f55df-a7cd-407f-bddc-33e5304b65a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Informações do notebook\n",
    "- Este notebook tem como objetivo realizar ingestões no Lake de arquivos disponibilizados\n",
    "-------------\n",
    "\n",
    "1. **Tipo do arquivo**: O parâmetro tem objetivo de informar o tipo de arquivo que será processado.\n",
    "     - Opções: \n",
    "          - ``csv``\n",
    "          - ``xlsx``\n",
    "\n",
    "2. **source_container**: O parâmetro tem objetivo de informar em qual container o arquivo está disponibilizado.\n",
    "     - Padrão: `transient`\n",
    "\n",
    "3. **source_directory**: O parâmetro tem objetivo de informar em qual a path que o arquivo está disponibilizado.\n",
    "\n",
    "4. **Arquivo de Origem**: O parâmetro tem objetivo de informar o nome do arquivo a ser processado.\n",
    "\n",
    "5. **Delimitador**: O parâmetro tem objetivo de informar qual o delimitador utilizado no arquivo do tipo ``csv`.\n",
    "     - Padrão: `;`\n",
    "\n",
    "6. **SkipFooter**: O parâmetro tem objetivo de informar quantas colunas serão puladas olhando de cima para baixo.\n",
    "     - Padrão: `N/A`\n",
    "\n",
    "7. **SkipRows**: O parâmetro tem objetivo de informar quantas colunas serão puladas olhando de baixo para cima.\n",
    "     - Padrão: `N/A`\n",
    "\n",
    "8. **Planilha**: O parâmetro tem objetivo de informar qual shet do arquivo do tipo ``xlsx`` será processado.\n",
    "     - Padrão: `N/A`\n",
    "\n",
    "9. **Coluna**: O parâmetro tem objetivo de informar quais (`colunas`) do arquivos serão lidas.\n",
    "     - Padrão: `full`\n",
    "\n",
    "10. **Table name**: O parâmetro tem objetivo de informar qual o nome da tabela a ser criada.\n",
    "     - Padrão: `N/A` -> Caso o falor for igual a `N/A` o processo irá considerar o valor do parâmetro ``Arquivo de Origem``\n",
    "     - Objetivo: Esse parâmetro tem objetivo de customizar o nome da tabela a ser criada.\n",
    "\n",
    "11. **Target Container**: O parâmetro tem objetivo de informar qual container será gravada a tabela a ser criada.\n",
    "     - Padrão: `bronze`\n",
    "\n",
    "12. **Write Table Model**: O parâmetro tem objetivo de informar qual será o tipo de carga que será utilizada na alimentação/criação da tabela.\n",
    "     - `overwrite`\n",
    "     - `append`\n",
    "\n",
    "13. **Target directory**: O parâmetro tem objetivo de informar qual o diretório a tabela será salva.\n",
    "     - Padrão: `bronze`\n",
    "\n",
    "14. **DataLake Name**: O parâmetro tem objetivo de informar qual o storage a tabela será salva.\n",
    "     - Padrão: `{Nome do Storage}`\n",
    "\n",
    "-------------\n",
    "\n",
    "###**Regras de tratamento**\n",
    "- O processo realizar uma serie de tratamento no nome das colunas e no nome da tabela a ser criada, sendo elas:\n",
    "  - Substituição do ` `  por `_`.\n",
    "  - Remoção de caracteres especiais.\n",
    "  - Reescrita para minusculo.\n",
    "  - Caso o valor da coluna seja `nulo` o processo colocará ``padrao_1``, ``padrao_2``...\n",
    "  - Caso o valor da coluna tenha somente número, o processo colocará o `n_` antes do número.\n",
    "\n",
    "- Para leitura de arquivo do tipo `csv` o processo utiliza o ``encoding = UTF-8``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "436cac41-e14f-41db-857a-2f4ca1296497",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pip Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bed67900-39cb-473d-9267-2f25c7b2015a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-storage-blob\n",
    "%pip install openpyxl\n",
    "%pip install fsspec adlfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9242c862-6f1e-4b00-8dae-4bc0b0c09ac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34420cd1-008c-42c7-9a5c-8aa89e2b7705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from io import BytesIO\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0c0bbce-b68b-46eb-b7f1-e7fc2a8d9774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d2577d1-25d7-48ec-aef9-4d09e39c34c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuração do logger\n",
    "logger = logging.getLogger('Ingestion xlsx')\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "848b2cc9-0fa3-4918-9488-7df8e8389241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parâmetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d75f891-71a9-45fb-bc6b-f0a554809e60",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parâmetros para leitura"
    }
   },
   "outputs": [],
   "source": [
    "## Definir container de Tipo do arquivo\n",
    "dbutils.widgets.dropdown(\"file_type\",\"csv\",[\"csv\", \"xlsx\"],\"1.Tipo do arquivo\")\n",
    "file_type = dbutils.widgets.get(\"file_type\")\n",
    "\n",
    "## Definir container de origem\n",
    "dbutils.widgets.text(\"source_container\",\"N/A\",\"2.Container de Origem\")\n",
    "source_container = dbutils.widgets.get(\"source_container\")\n",
    "\n",
    "## Definir diretório de origem\n",
    "dbutils.widgets.text(\"source_directory\",\"N/A\",\"3.Diretório de Origem\")\n",
    "source_directory = dbutils.widgets.get(\"source_directory\")\n",
    "\n",
    "## Definir arquivo de origem\n",
    "dbutils.widgets.text(\"source_file\",\"N/A\",\"4.Arquivo de Origem\")\n",
    "source_file = dbutils.widgets.get(\"source_file\")\n",
    "\n",
    "## Definir coluna\n",
    "dbutils.widgets.text(\"delimitador\",\"N/A\",\"5.Delimitador\")\n",
    "delimitador = dbutils.widgets.get(\"delimitador\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b6601d1-0932-4c27-9d82-1a2e36a64980",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parâmetros para tratamento"
    }
   },
   "outputs": [],
   "source": [
    "## Definir skipfooter\n",
    "dbutils.widgets.text(\"skipfooter\",\"N/A\",\"6.Rodapé a Ignorar\")\n",
    "skipfooter = dbutils.widgets.get(\"skipfooter\")\n",
    "\n",
    "## Definir skiprows\n",
    "dbutils.widgets.text(\"skiprows\",\"N/A\",\"7.Linhas a Ignorar\")\n",
    "skiprows = dbutils.widgets.get(\"skiprows\")\n",
    "\n",
    "## Definir planilha\n",
    "dbutils.widgets.text(\"planilha\",\"N/A\",\"8.Planilha\")\n",
    "planilha = dbutils.widgets.get(\"planilha\")\n",
    "\n",
    "## Definir coluna\n",
    "dbutils.widgets.text(\"coluna\",\"full\",\"9.Coluna\")\n",
    "coluna = dbutils.widgets.get(\"coluna\")\n",
    "if coluna == 'full':\n",
    "    coluna = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db7c400e-3417-49c4-9b26-4f94e6dc787b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parâmetros para save"
    }
   },
   "outputs": [],
   "source": [
    "## Definir table_name\n",
    "dbutils.widgets.text(\"table_name\",\"N/A\",\"10.Table name\")\n",
    "table_name = dbutils.widgets.get(\"table_name\")\n",
    "if table_name == 'N/A':\n",
    "    table_name = source_file.replace(\".csv\", \"\").replace(\".txt\", \"\").replace(\".xlsx\", \"\")\n",
    "else:\n",
    "    table_name = dbutils.widgets.get(\"table_name\")\n",
    "\n",
    "## Definir target container\n",
    "dbutils.widgets.text(\"target_container\",\"N/A\",\"11.Target Container\")\n",
    "target_container = dbutils.widgets.get(\"target_container\")\n",
    "\n",
    "## Definir write table model\n",
    "dbutils.widgets.text(\"write_table_model\",\"N/A\",\"12.Write Table Model\")\n",
    "write_table_model = dbutils.widgets.get(\"write_table_model\")\n",
    "\n",
    "## Definir write table model\n",
    "dbutils.widgets.text(\"target_directory\",\"N/A\",\"13.Target directory\")\n",
    "target_directory = dbutils.widgets.get(\"target_directory\")\n",
    "\n",
    "## Definir write table model\n",
    "dbutils.widgets.text(\"data_lake_name\",\"N/A\",\"14.DataLake Name\")\n",
    "data_lake_name = dbutils.widgets.get(\"data_lake_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94bfe6ce-b3db-4b1e-b12b-bf19d56ffa26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Client - Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5533eb0-a63d-4b75-9462-a6a4eb342ec9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obter Key datalake\n",
    "secret_value = dbutils.secrets.get(scope=\"scope-databricks\", key=\"secret-key-datalake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef750bf5-1cd9-41e2-a957-88cb088e25e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if file_type == 'xlsx':\n",
    "    ## Criando a connextion_string\n",
    "    connection_string = f\"DefaultEndpointsProtocol=https;AccountName={data_lake_name};AccountKey={secret_value};EndpointSuffix=core.windows.net\"\n",
    "    \n",
    "    # Create a BlobServiceClient\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    \n",
    "    # Get the blob client\n",
    "    blob_client = blob_service_client.get_blob_client(container=source_container, blob=f\"{source_directory}/{source_file}\")\n",
    "\n",
    "    ## Criando caminho para o path\n",
    "    logger.info('Criando caminho para o path:', source_file)\n",
    "    path = f\"abfss://{source_container}@{data_lake_name}.dfs.core.windows.net/{source_directory}/{source_file}\"\n",
    "    #path = \"/mnt/transient/\" + source_directory + \"/\" + source_file\n",
    "\n",
    "elif file_type == 'csv':\n",
    "    # Configurando a chave de acesso no Spark\n",
    "    spark.conf.set(f\"fs.azure.account.key.{data_lake_name}.dfs.core.windows.net\", secret_value)\n",
    "\n",
    "    ## Criando caminho para o path\n",
    "    logger.info('Criando caminho para o path:', source_file)\n",
    "    path = f\"abfss://{source_container}@{data_lake_name}.dfs.core.windows.net/{source_directory}/{source_file}\"\n",
    "    #path = \"/mnt/transient/\" + source_directory + \"/\" + source_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ac42c3-def3-4706-bd22-5100c8faa0bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DEFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd51bd31-7a03-4de7-8034-79ac53bc1677",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Formatação"
    }
   },
   "outputs": [],
   "source": [
    "## Remove os acentos\n",
    "def remove_accents(input_str):\n",
    "    logger.info(\"Iniciando remoção de acentos.\")\n",
    "    nfkd_form = unicodedata.normalize('NFD', input_str)\n",
    "    without_accents = re.sub(r'[\\u0300-\\u036f]', '', nfkd_form)\n",
    "    logger.info(\"Remoção de acentos concluída.\")\n",
    "    return without_accents\n",
    "\n",
    "\n",
    "## Formata nome das colunas\n",
    "def clean_column_names(df: DataFrame) -> DataFrame:\n",
    "    logger.info(\"Iniciando formatação dos nomes das colunas.\")\n",
    "    new_column_names = {}\n",
    "    for column in df.columns:\n",
    "        new_column = remove_accents(re.sub(r\"[. ()#\\+\\-\\\\/]\", \"_\", column.lower().replace(\"º\", \"o\").replace(\"ª\", \"a\").replace(\"%\", \"perc\")))\n",
    "        new_column = re.sub(r'[^a-zA-Z0-9_]', '', new_column)\n",
    "        new_column = new_column.replace(' ', '_')\n",
    "        new_column_names[column] = new_column\n",
    "    \n",
    "    for old_col, new_col in new_column_names.items():\n",
    "        df = df.withColumnRenamed(old_col, new_col)\n",
    "    \n",
    "    logger.info(\"Formatação dos nomes das colunas concluída.\")\n",
    "    return df\n",
    "\n",
    "## Formatar dados\n",
    "def format_data(df):\n",
    "    logger.info(\"Iniciando formatação dos dados.\")\n",
    "    df = clean_column_names(df)\n",
    "    logger.info(\"Formatação dos dados concluída.\")\n",
    "    return df\n",
    "\n",
    "## Remover caracteres\n",
    "def remove_caracteres(name):\n",
    "    logger.info(\"Iniciando remoção de caracteres.\")\n",
    "    ## Removendo acentos\n",
    "    file = remove_accents(name)\n",
    "    ## Removendo caracteres especiais\n",
    "    file = remove_accents(re.sub(r\"[. ()#\\+\\-\\\\/]\", \"_\", file.lower().replace(\"º\", \"o\").replace(\"ª\", \"a\").replace(\"%\", \"perc\")))\n",
    "    file = re.sub(r'[^a-zA-Z0-9_]', '', file)\n",
    "    file = file.replace(' ', '_')\n",
    "    logger.info(\"Remoção de caracteres concluída.\")\n",
    "    return file\n",
    "\n",
    "def rename_duplicate_columns(df_file):\n",
    "    # Obter as colunas do DataFrame\n",
    "    columns = df_file.columns\n",
    "    column_counts = {}\n",
    "\n",
    "    # Contar a ocorrência de cada coluna\n",
    "    for col in columns:\n",
    "        column_counts[col] = column_counts.get(col, 0) + 1\n",
    "\n",
    "    # Lista para armazenar os novos nomes das colunas\n",
    "    new_columns = []\n",
    "\n",
    "    # Renomear as colunas duplicadas com um sufixo sequencial\n",
    "    for col in columns:\n",
    "        if column_counts[col] > 1:\n",
    "            # Incrementa o contador para colunas duplicadas\n",
    "            column_counts[col] -= 1\n",
    "            new_col_name = f\"{col}_{column_counts[col]}\"\n",
    "            new_columns.append(new_col_name)\n",
    "        else:\n",
    "            # Se não for duplicado, mantém o nome original\n",
    "            new_columns.append(col)\n",
    "\n",
    "    # Renomear as colunas no DataFrame\n",
    "    df_file = df_file.toDF(*new_columns)\n",
    "    \n",
    "    return df_file\n",
    "\n",
    "def renomeia_colunas_com_numeros(df_file):\n",
    "    \"\"\"\n",
    "    Função que renomeia colunas que possuem apenas números, adicionando o prefixo 'n_'.\n",
    "    \"\"\"\n",
    "    # Obter os nomes das colunas do DataFrame\n",
    "    colunas = df_file.columns\n",
    "    \n",
    "    # Lista para armazenar os novos nomes das colunas\n",
    "    novas_colunas = []\n",
    "    \n",
    "    # Loop pelas colunas para verificar se o nome é numérico\n",
    "    for coluna in colunas:\n",
    "        # Verificar se o nome da coluna é numérico\n",
    "        if coluna.isdigit():\n",
    "            novas_colunas.append(f\"n_{coluna}\")  # Adiciona 'n_' no início\n",
    "        else:\n",
    "            novas_colunas.append(coluna)  # Mantém o nome da coluna como está\n",
    "    \n",
    "    # Renomeia as colunas do DataFrame\n",
    "    for old_name, new_name in zip(colunas, novas_colunas):\n",
    "        df_file = df_file.withColumnRenamed(old_name, new_name)\n",
    "    \n",
    "    return df_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c56f458-3a4f-44e7-84c8-458555ab061f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "csv_ajuste_file"
    }
   },
   "outputs": [],
   "source": [
    "# Função para gerar nomes de colunas de forma alfabética\n",
    "def generate_column_names(n):\n",
    "    # Gera os nomes das colunas baseados no padrão A, B, ..., Z, AA, AB, ...\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        column_name = \"\"\n",
    "        num = i\n",
    "        while num >= 0:\n",
    "            column_name = chr(num % 26 + 65) + column_name\n",
    "            num = num // 26 - 1\n",
    "        result.append(column_name)\n",
    "    return result\n",
    "\n",
    "def csv_ajuste_file(df_csv, sr, sf, cl):\n",
    "\n",
    "    # Removendo linhas - skiprows\n",
    "    if sr != 'N/A':\n",
    "        windowSpec = Window.orderBy(lit(1))\n",
    "        df_with_index = df_csv.withColumn(\"row_num\", row_number().over(windowSpec))\n",
    "        df_csv = df_with_index.filter(col(\"row_num\") > sr).drop(\"row_num\")\n",
    "\n",
    "    if sf != 'N/A':\n",
    "        df_csv = df_csv.limit(df_csv.count() - int(sf))\n",
    "\n",
    "    if coluna != 'full':\n",
    "        # Gerar os novos nomes das colunas com a função customizada\n",
    "        new_columns = generate_column_names(len(df_csv.columns))\n",
    "        column_mapping = dict(zip(df_csv.columns, new_columns))\n",
    "        df_csv = df_csv.select([df_csv[col].alias(column_mapping[col]) for col in df_csv.columns])\n",
    "\n",
    "        # Selecionando as colunas\n",
    "        colunas_selecionadas_lista = cl.split(',')\n",
    "        colunas_final = [coluna for coluna in df_csv.columns if coluna in colunas_selecionadas_lista]\n",
    "        df_csv = df_csv.select(*colunas_final)\n",
    "\n",
    "    # Verifica se o DataFrame tem dados antes de acessar a primeira linha\n",
    "    if df_csv.count() > 0:\n",
    "\n",
    "        # Remover a primeira linha para não incluí-la no DataFrame final\n",
    "        windowSpec = Window.orderBy(lit(1))\n",
    "        df_with_index = df_csv.withColumn(\"row_num\", row_number().over(windowSpec))\n",
    "        df_without_header = df_with_index.filter(col(\"row_num\") > 1).drop(\"row_num\")\\\n",
    "\n",
    "        # Capturar a primeira linha do DataFrame\n",
    "        first_row = df_csv.first()\n",
    "        new_columns = []\n",
    "\n",
    "        for i, value in enumerate(first_row):\n",
    "            if value is None:\n",
    "                # Substituir valores nulos por 'padrão1', 'padrão_1', 'padrão_2', etc.\n",
    "                new_columns.append(f\"padrão_{i+1}\" if i > 0 else \"padrão1\")\n",
    "            else:\n",
    "                new_columns.append(str(value))\n",
    "\n",
    "        df_csv = df_without_header.toDF(*new_columns)\n",
    "\n",
    "    return df_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2dd30c7-78ab-4e5e-b8cd-8363237d1be7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read file - xlsx"
    }
   },
   "outputs": [],
   "source": [
    "def read_file_xlsx(path, pl, cl, sr, sf):\n",
    "    logger.info(\"Iniciando leitura do arquivo.\")\n",
    "\n",
    "    ## Tratamento Path\n",
    "    #path = path.replace('dbfs:', '')\n",
    "\n",
    "    ## Tratamento planilha\n",
    "    if pl == 'N/A':\n",
    "        pl = 0\n",
    "\n",
    "    ## Tratamento coluna\n",
    "    if cl == 'full':\n",
    "        cl = None\n",
    "\n",
    "    ## Tratamento primeira linha\n",
    "    if sr == 'N/A':\n",
    "        sr = 0\n",
    "    else:\n",
    "        sr = int(sr)\n",
    "\n",
    "    ## Tratamento ultima linha\n",
    "    if sf == 'N/A':\n",
    "        sf = 0\n",
    "    else:\n",
    "        sf = int(sf)\n",
    "\n",
    "    ## Leitura do arquivo\n",
    "    logger.info(\"Iniciando leitura do arquivo Excel.\")\n",
    "    ex = pd.read_excel(path, dtype=str, na_values=None, sheet_name=pl, index_col=None, header=None, usecols=cl, skiprows=sr, skipfooter=sf)\n",
    "    logger.info(\"Leitura do arquivo Excel concluída.\")\n",
    "    ex.columns = ex.iloc[0]\n",
    "    ex = ex.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "    ## Converter DF Spark\n",
    "    logger.info(\"Convertendo DataFrame para Spark.\")\n",
    "    df = spark.createDataFrame(ex)\n",
    "    logger.info(\"Conversão para DataFrame Spark concluída.\")\n",
    "\n",
    "    logger.info(\"Leitura do arquivo concluída.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a02249-5797-4778-bf8e-19ec7e6647d8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read file - csv"
    }
   },
   "outputs": [],
   "source": [
    "def read_file_csv(path, dl):\n",
    "    logger.info(f\"Lendo arquivo: {path}\")\n",
    "\n",
    "    ## Leitura do arquivo\n",
    "    df = (\n",
    "                spark.read.format(\"csv\")\n",
    "                .option(\"delimiter\", dl)\n",
    "                .option(\"header\", \"false\")\n",
    "                .option(\"inferSchema\", \"false\") \\\n",
    "                .option(\"encoding\", \"UTF-8\")\n",
    "                .load(path)\n",
    "            )\n",
    "    \n",
    "    logger.info(\"Arquivo lido com sucesso.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae7bfbd8-2fc1-4f50-8c23-fac521de698c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Extract / Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91ae1fbf-e4b8-431c-b53b-aba8a04f9e65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if file_type == 'csv':\n",
    "    ## Realizando leitura do arquivo\n",
    "    logger.info('Realizando tratamento no nome das colunas da tabela: %s', table)\n",
    "    df = read_file_csv(path, delimitador)\n",
    "    \n",
    "elif file_type == 'xlsx':\n",
    "    # Download the blob content to a BytesIO object\n",
    "    blob_data = blob_client.download_blob().readall()\n",
    "    excel_data = BytesIO(blob_data)\n",
    "    \n",
    "    ## Realizando leitura do arquivo\n",
    "    logger.info('Realizando tratamento no nome das colunas da tabela: %s', table)\n",
    "    df = read_file_xlsx(excel_data, planilha, coluna, skiprows, skipfooter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "addc5de5-96f4-4f96-8fc7-fa53b8ed98a1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Csv - Ajuste DF"
    }
   },
   "outputs": [],
   "source": [
    "if file_type == 'csv':\n",
    "    df = csv_ajuste_file(df, skiprows, skipfooter, coluna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae07fbdf-2be8-4027-9d24-3fc2c18de590",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Tratando tabela"
    }
   },
   "outputs": [],
   "source": [
    "## Realizando tratamento na tabela\n",
    "logger.info('Realizando tratamento no nome das colunas da tabela: %s', table)\n",
    "df = format_data(df)\n",
    "\n",
    "## Tratando nome da tabela\n",
    "logger.info('Tratando nome da tabela: %s', table)\n",
    "table_name = remove_caracteres(table_name)\n",
    "\n",
    "## Tratando colunas duplicadas\n",
    "logger.info('Tratando colunas duplicadas: %s', table)\n",
    "df = rename_duplicate_columns(df)\n",
    "\n",
    "## Tratando colunas númericas\n",
    "df = renomeia_colunas_com_numeros(df)\n",
    "\n",
    "## Tratando valores nulas\n",
    "df = df.dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e66e3db-5336-41f0-b458-8176f1249066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f955c77-c328-4d2e-a729-4bdb52f5adbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Criando Path delta\n",
    "logger.info(\"Criando path para salvar o Delta file.\")\n",
    "#deltaFile = f'/mnt/{target_directory}/{source_directory}/{table_name}'\n",
    "delta_file = f\"abfss://{target_container}@{data_lake_name}.dfs.core.windows.net/{target_directory}/{table_name}/\"\n",
    "\n",
    "## Definir nome da tabela delta\n",
    "delta_table_name = f\"{environment}.{target_container}.{table_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d838d09e-e0b6-4c8b-8831-d92501001fd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Save table\n",
    "logger.info('Realizando o save da tabela:', table)\n",
    "if df.count() > 0:\n",
    "    (\n",
    "        df.write\n",
    "            .mode(write_table_model) \\\n",
    "            .format(\"delta\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(delta_table_name, path = deltaFile)\n",
    "    )\n",
    "else:\n",
    "    logger.info('Realizando o save na tabela: %s', table)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_ingestion_file",
   "widgets": {
    "coluna": {
     "currentValue": "N/A",
     "nuid": "ce3a78e5-dfa0-44bd-98aa-8f08a3df7744",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "full",
      "label": "9.Coluna",
      "name": "coluna",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "full",
      "label": "9.Coluna",
      "name": "coluna",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "data_lake_name": {
     "currentValue": "N/A",
     "nuid": "f337d2d6-e5e5-4d9b-96bf-e9e0dcfe2187",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "N/A",
      "label": "16.DataLake Name",
      "name": "data_lake_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "N/A",
      "label": "16.DataLake Name",
      "name": "data_lake_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "delimitador": {
     "currentValue": "N/A",
     "nuid": "05ff34ee-9e5f-4288-b107-8bdb16643fb1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "N/A",
      "label": "5.Delimitador",
      "name": "delimitador",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "N/A",
      "label": "5.Delimitador",
      "name": "delimitador",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "file_type": {
     "currentValue": "csv",
     "nuid": "34f2814a-6272-4261-bddb-2fe42acadc9b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "csv",
      "label": "1.Tipo do arquivo",
      "name": "file_type",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "csv",
        "xlsx"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "csv",
      "label": "1.Tipo do arquivo",
      "name": "file_type",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "csv",
        "xlsx"
       ]
      }
     }
    },
    "merge_columns": {
     "currentValue": "N/A",
     "nuid": "1f0d6b9a-eb9a-40e8-8415-1d70e9dbb4b1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "N/A",
      "label": "13.Merge Columns",
      "name": "merge_columns",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "N/A",
      "label": "13.Merge Columns",
      "name": "merge_columns",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "planilha": {
     "currentValue": "N/A",
     "nuid": "f51441ad-c458-4417-9b2f-616fa4ee56db",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "N/A",
      "label": "8.Planilha",
      "name": "planilha",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "N/A",
      "label": "8.Planilha",
      "name": "planilha",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "project_name": {
     "currentValue": "N/A",
     "nuid": "456835dc-d23b-4f4d-b496-d8c2fd49a1ea",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "N/A",
      "label": "17.DataLake Name",
      "name": "project_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "N/A",
      "label": "17.DataLake Name",
      "name": "project_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "replace_column": {
     "currentValue": "N/A",
     "nuid": "4626e550-502f-4af1-a287-f88e36ce73db",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "N/A",
      "label": "18.Replace column",
      "name": "replace_column",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "N/A",
      "label": "18.Replace column",
      "name": "replace_column",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "skipfooter": {
     "currentValue": "N/A",
     "nuid": "cd1d80de-ee30-4047-8ee0-c1ae1d38502e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "N/A",
      "label": "6.Rodapé a Ignorar",
      "name": "skipfooter",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "N/A",
      "label": "6.Rodapé a Ignorar",
      "name": "skipfooter",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "skiprows": {
     "currentValue": "N/A",
     "nuid": "eee1f666-d25e-44ea-9575-300978a6a541",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "N/A",
      "label": "7.Linhas a Ignorar",
      "name": "skiprows",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "N/A",
      "label": "7.Linhas a Ignorar",
      "name": "skiprows",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "source_container": {
     "currentValue": "N/A",
     "nuid": "3fddb5a5-714f-4c17-a84a-31e3737620d5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "N/A",
      "label": "2.Container de Origem",
      "name": "source_container",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "N/A",
      "label": "2.Container de Origem",
      "name": "source_container",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "source_directory": {
     "currentValue": "N/A",
     "nuid": "243795ea-44de-4fb5-bd7a-24a621097153",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "N/A",
      "label": "3.Diretório de Origem",
      "name": "source_directory",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "N/A",
      "label": "3.Diretório de Origem",
      "name": "source_directory",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "source_file": {
     "currentValue": "N/A",
     "nuid": "06aeac8c-b834-4368-b050-daecc527cbc5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "N/A",
      "label": "4.Arquivo de Origem",
      "name": "source_file",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "N/A",
      "label": "4.Arquivo de Origem",
      "name": "source_file",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "table_name": {
     "currentValue": "N/A",
     "nuid": "4597e8ec-3126-4ea8-b9a0-cd8c2ac5b242",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "N/A",
      "label": "11.Table name",
      "name": "table_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "N/A",
      "label": "11.Table name",
      "name": "table_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_container": {
     "currentValue": "N/A",
     "nuid": "157da777-ce81-4a1b-b0a0-c12126c5a65d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "N/A",
      "label": "12.Target Container",
      "name": "target_container",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "N/A",
      "label": "12.Target Container",
      "name": "target_container",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_directory": {
     "currentValue": "N/A",
     "nuid": "471ba2fb-c387-41b4-b239-dae9fdf40275",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "N/A",
      "label": "15.Target directory",
      "name": "target_directory",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "N/A",
      "label": "15.Target directory",
      "name": "target_directory",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "write_table_model": {
     "currentValue": "N/A",
     "nuid": "500f808a-8a5a-42d6-864b-06565fca1797",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "N/A",
      "label": "14.Write Table Model",
      "name": "write_table_model",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "N/A",
      "label": "14.Write Table Model",
      "name": "write_table_model",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
